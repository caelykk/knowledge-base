
# Django

## Что такое Middleware, для чего, как реализуется

Middleware – особый объект, который обычно изменяет входящий запрос или исходящий ответ. Например, добавляет заголовки, делает предварительные проверки. Middleware нужен, когда требуется подвергнуть обработке все запросы приложения.

На уровне языка это объект с методами `process_request` и `process_response`. Методы должны вернуть принятый объект (запрос или ответ) для дальнейшей обработки или выкинуть исключение, если что-то не в порядке. В этом случает дальнейшая обработка прекращается.

Чтобы включить Middleware, достаточно добавить путь к нему в список `MIDDLEWARE`.

## Назовите основные мидлвари. Зачем они нужны

- `SessionMiddleware` – поддержка сессий. Добавляет в запрос объект `session`
- `CsrfViewMiddleware` – проверяет, что POST-запросы отправлены с текущего домена
- `AuthenticationMiddleware` – авторизует пользователя. Добавляет в запрос поле `user`
- `MessageMiddleware` – передает пользователю короткие сообщения

## Опишите алгоритм работы CSRF middleware

На каждый запрос система генерирует уникальный токен и выставляет его в куках. В каждой форме размещается скрытое поле `csrf-token` с этим же токеном. При отправке формы методом `POST` Джанго проверяет, что поле формы и значение в куках совпадают. Если нет, это значит, что запрос подделан или отправлен с другого домена.

Чтобы освободить какую-то вьюху от проверки (если это API, например), достаточно обернуть ее декоратором `csrf_except`.

Промежуточный слой CSRF и шаблонный тег предоставляют легкую-в-использовании защиту против Межсайтовой подделки запроса. Этот тип атак случается, когда злонамеренный Web сайт содержит ссылку, кнопку формы или некоторый javascript, который предназначен для выполнения некоторых действий на вашем Web сайте, используя учетные данные авторизованного пользователя, который посещал злонамеренный сайт в своем браузере. Сюда также входит связанный тип атак, ‘login CSRF’, где атакуемый сайт обманывает браузер пользователя, авторизируясь на сайте с чужими учетными данными.

Первая защита против CSRF атак - это гарантирование того, что GET запросы (и другие ‘безопасные’ методы, определенные в 9.1.1 Safe Methods, HTTP 1.1, [RFC 2616#section-9.1.1](https://tools.ietf.org/html/rfc2616.html#section-9.1.1)) свободны от побочных эффектов. Запросы через ‘небезопасные’ методы, такие как POST, PUT и DELETE могут быть защищены при помощи шагов, описанных ниже.

*Как это работает:*

CSRF базируется на следующих вещах:

1. CSRF кука, которая устанавливается как случайное число (сессия независимого случайного слова, как это еще иногда называют), к которой другие сайты не будут иметь доступа. Эта кука устанавливается при помощи CsrfViewMiddleware. Она должно быть постоянной, но так как нет способа установить куки, у которых никогда не истекает время жизни, то она отправляется с каждым ответом, который вызывал django.middleware.csrf.get_token() (функция использовалась внутри для получения CSRF токена).

2. Все POST формы содержат скрытое поле ‘csrfmiddlewaretoken’. Значение поля равно CSRF куке. Эта часть выполняет шаблонным тегом.
3. Все HTTP запросы, которые не GET, HEAD, OPTIONS или TRACE, должны содержать CSRF куку, и поле ‘csrfmiddlewaretoken’ с правильным значением. Иначе пользователь получит 403 ошибку. Эта проверка выполняется в CsrfViewMiddleware.
4. В дополнение для HTTPS запросов в CsrfViewMiddleware проверяется “referer”(источник запроса). Это необходимо для предотвращения MITM-атаки(Man-In-The-Middle), которая возможна при использовании HTTPS и токена не привязанного к сессии, т.к. клиенты принимают(к сожалению) HTTP заголовок ‘Set-Cookie’, несмотря на то, что коммуникация с сервером происходит через HTTPS. (Такая проверка не выполняется для HTTP запросов т.к. “Referer” заголовок легко подменить при использовании HTTP.) Если указана настройка CSRF_COOKIE_DOMAIN, значение “referer” будет сравниваться с этим значением. Значение поддерживает под-домены. Например, CSRF_COOKIE_DOMAIN = '.example.com' позволить отправлять POST запросы с www.example.com и api.example.com. Если настройка не указана, “referer” должен быть равен HTTP заголовку Host. Чтобы расширить список доступных доменов, кроме текущего хоста и домена кук, используйте CSRF_TRUSTED_ORIGINS.

Такой подход гарантирует, что только формы, отправленные с доверенных доменов, могут передавать POST данные.

GET игнорируются сознательно (и все другие запросы, которые считаются “безопасными” в соответствии с RFC 2616). Эти запросы никогда не должны выполнять каких-либо потенциально опасные действия, и CSRF атаки через GET запрос должен быть безвредным. RFC 2616 определяет POST, PUT и DELETE как “небезопасные”.

## Что такое сигналы? Зачем нужны? Назовите основные

Сигналы – это события в экосистеме Джанго. С помощью сигналов подсистемы оповещают приложение о том, что случилось. Чтобы читать сигналы, программист регистрирет обработчики сигналов. Сигналы распространяются синхронно. Это значит, подписав на один сигнал сотню обработчиков, мы увеличим время, необходимое на отдачу ответа.

Основные сигналы это начало запроса и его окончание, перед сохранением модели и после, обращение к базе данных.

**Важно:** сигналы моделей работают поштучно, то есть для одной модели. При пакетной обработке, например, `queryset.all().delete()` или `queryset.all().update({'foo'=42})`, события об удалении или изменения не будут вызваны.

## Как реализуется связь m2m на уровне базы данных

Если есть модели A и B со связью многие ко многим, то создается таблица-мост с именем `a_to_b`, которая хранит ключ на A, ключ на B и дополнительные сведения, например, время, когда была создана связь. Эта таблица сцепляется с A и B оператором `JOIN`.

## Чем лучше отправлять форму — GET или POST

Форму можно отправлять обоими способами. В первом случае переменные прикрепляются к строке запроса после вопросительного знака. Во втором – передаются в теле запроса.

Техническое ограничение метода GET в том, что им нельзя передать файл, в отличие от POST.

Форму желательно передавать методом POST по следующим причинам:

- GET-запросы могут быть кешированы, особенно в браузерах семейства IE
- GET-запросы оседают в логах провайдера, сервера, истории браузера. Пароль и логин в таком случае может засветиться во многих местах
- некоторые вирусы отслеживают содержимое адресной строки и пересылают третьим лицам.

## Как работает Serializer в Django REST Framework

Serializer преобразует информацию, хранящуюся в базе данных и определенную с помощью моделей Django, в формат, который легко и эффективно передается через API.

Модели Django интуитивно представляют данные, хранящиеся в базе, но API должен передавать информацию в менее сложной структуре. Хотя данные будут представлены как экземпляры классов Model, их необходимо перевести в формат JSON для передачи через API.

Сериализатор DRF производит это преобразование. Когда пользователь передает информацию (например, создание нового экземпляра) через API, сериализатор берет данные, проверяет их и преобразует в нечто, что Django может сложить в экземпляр модели. Аналогичным образом, когда пользователь обращается к информации через API, соответствующие экземпляры передаются в сериализатор, который преобразовывает их в формат, который может быть легко передан пользователю как JSON.

Наиболее распространенной формой, которую принимает сериализатор DRF, является тот, который привязан непосредственно к модели Django:

```python
class ThingSerializer(serializers.ModelSerializer):
  class Meta:
    model = Thing
    fields = (‘name’, )
```

Настройки fields позволяют точно указать, какие поля доступны этому сериализатору. В качестве альтернативы, может быть установлен exclude вместо fields, которое будет включать все поля модели, кроме тех, которые указаны в exclude.

Сериализаторы — это невероятно гибкий и мощный компонент DRF. Хотя подключение сериализатора к модели является наиболее распространенным, сериализаторы могут использоваться для создания любой структуры данных Python через API в соответствии с определенными параметрами.

## Что такое Meta в классах Django и для чего нужен

Django во многом работает через метаклассы.

Если коротко, то метаклассы - это классы, которые конструируют другие классы. Объявляются они через атрибут класса `__metaclass__` (в джанге через слой совместимости с python 3 через модуль six до версии 2).

Поэтому когда Django конструирует ваш класс, она делает это с помощью своего метакласса. Чтобы при конструировании ей знать какие-то параметры вашего класса, ну, например модель или поля в вашем случае, она ищет в вашем классе класс с названием Meta.

Вообще вся эта магия с метаклассами очень важна в джанге и поэтому лучше саму логику становления класса не переопределять.

Если взять за пример Serializer, то можно посмотреть на код джанги:

```python
@six.add_metaclass(SerializerMetaclass)
class Serializer(BaseSerializer):
  ...
```

SerializerMetaclass - это тот самый метакласс, который конструирует класс ModelForm.

## За что отвечает Meta в сериализаторе

В классе Meta сериализатора можно задать модель по которой будет создан сериализатор, поля, которые будут включены (или exclude для исключения), `list_serializer_class`, например для того чтобы задать специфическую валидацию списков и тд.

## Какая разница в быстродействии между django и Flask (и почему)

Насчет быстродействия затрудняюсь ответить, потому что это довольно каверзный вопрос, тестов лично я не проводил. Но, что касается отличий этих двух фреймворков:

- Flask предоставляет простоту, гибкость и аккуратность в работе, позволяя пользователю самому выбирать, как реализовать те или иные вещи.
- Django предоставляет пакет «все включено»: у вас есть панель админа, интерфейсы баз данных, ORM, и структура каталогов для ваших приложений и проектов.

Под каждую задачу нужно брать свой инструмент, Django хорошо подойдет для новостных сайтов, блогов и тд, благодаря тому что у него уже из коробки есть многое (в том числе админка), да и создавался он именно под такой тип сайтов. Flask же из коробки напротив, практически ничего не имеет и лучше подойдет для каких-либо микросервисов или приложений для которых стек технологий с которыми поставляется Django не подходит.

## Как в django работает система аутентификации

Django поставляется с системой аутентификации пользователей. Она обеспечивает пользовательские аккаунты, группы, права и сессии на основе куки.

Система аутентификации Django отвечает за оба аспекта: аутентификацию и авторизацию. Если коротко, то аутентификация проверяет пользователя, а авторизация определяет, что аутентифицированный пользователь может делать. Далее термин “аутентификация” будет использоваться для обозначения обоих аспектов ([User authentication in Django](https://docs.djangoproject.com/en/2.2/topics/auth/)).

Система аутентификации состоит из:

- Пользователей
- Прав: Бинарные (да/нет) флаги, определяющие наличие у пользователя права выполнять определённые действия.
- Групп: Общий способ назначения меток и прав на множество пользователей.
- Настраиваемой системы хеширования паролей
- Инструментов для форм и представлений для аутентификации пользователей или для ограничения доступа к контенту
- Системы плагинов

Аутентификационная система Django старается быть очень простой и не предоставляет некоторые фичи, распространённые в других системах веб аутентификации. Такие фичи реализованы в сторонних пакетах:

- Проверка сложности пароля
- Ограничение попыток входа
- Аутентификация через сторонние сервисы (OAuth, например)

# Веб-разработка

## Что такое CGI. Плюсы, минусы

Common Gateway Interface. Соглашение о том, как веб-сервер взаимодействует с программой, написанной на каком-то языке. Веб-сервер запускает программу как исполняемый файл. Параметры запроса, например, метод, путь, заголовки и т.д. передаются через переменные окружения.

Программа должна прочитать эти переменные и записать в стандартный поток вывода HTTP-ответ.

Плюсы:

- Протокол не накладывает условия на язык, на котором написана программа. Это     может быть и скрипт, и бинарный файл.
- Протокол экстремально прост.
- Программа не хранит состояние, что удобно для отладки.

Минусы:

- Запуск процесса ОС на каждый запрос отрабатывает очень медленно.
- Передача данных через `stdout` медленней юникс-сокетов.

## Как защитить куки от воровства и от подделки

Зависит от того, насколько строгие критерии безопасности на сайте. Если в куках хранятся вспомогательные данные, например, индекс последнего выбранного в дропдауне элемента, правилами ниже можно пренебречь.

Для платежных систем, сайтов с приватными данными приведенные правила обязательны.

- Выставлять кукам флаг `httponly`. Браузер не даст прочесть и изменить такие куки на клиенте Джаваскриптом.
- Использовать флаг `secure`. Куки будут переданы только по безопасному соединению.
- Устанавливать короткий срок жизни куки.
- Устанавливать короткий срок сессии на сервере.
- Добавлять в ключ сессии заголовок `User-Agent`. Тогда если украсть куки и установить на другой машине, ключ сессии будет другим.
- Аналогично пункту выше, но добавлять `IP` пользователя.
- Подписывать куки секретным ключом. Добавлять поле `sig`, которое равно `HMAC-SHA1(cookie-body, secret_key)`. На сервере проверять, что подпись совпадает.

## Какая разница между аутентификацией и авторизацией

**Идентификация** (от латинского identifico — отождествлять): присвоение субъектам и объектам идентификатора и / или сравнение идентификатора с перечнем присвоенных идентификаторов. Например, представление человека по имени отчеству - это идентификация.

**Аутентификация** (от греческого: αυθεντικός ; реальный или подлинный): проверка соответствия субъекта и того, за кого он пытается себя выдать, с помощью некой уникальной информации (отпечатки пальцев, цвет радужки, голос и тд.), в простейшем случае - с помощью имени входа и пароля.

**Авторизация** - это проверка и определение полномочий на выполнение некоторых действий (например, чтение файла /var/mail/eltsin) в соответствии с ранее выполненной аутентификацией.

Все три процедуры взаимосвязаны:

1. Сначала определяют имя (логин или номер) – идентификация
2. Затем проверяют пароль (ключ или отпечаток пальца) – аутентификация
3. И в конце предоставляют доступ – авторизация

## Что такое XSS. Примеры. Как защитить приложение

XSS – межсайтовые запросы. Страница, подверженная уязвимости, вынуждает пользователя выполнить запрос к другой странице, либо запустить нежелательный js-код.

Например, пользователь отправил комментарий, в котором был код:

```html
<script>alert('foo');</script>
```

Движок сайта не фильтрует текст комментария, поэтому тег `<script>` становится частью страницы и исполняется браузером. Каждый, кто зайдет на страницу с опасным комментарием, увидит всплывающее окно с тестом `foo`.

Другой пример. Страница поиска принимает поисковой терм `q`. В заголовке фраза “Результат поиска по запросу” + текст параметра. Если не экранировать параметр, то запрос `/search?q=<script>alert('foo');</script>` приведет к аналогичному результату.

Зная, что страница выполняет js-код, хакер может подгрузить на страницу контекстную рекламу, баннеры, заставить браузей перейти на любую страницу, похитить куки.

Уязвимость устраняется экранированием небезопасных символов, чисткой (санацией) HTML-тегов.

## REST & SOAP

### Что такое REST

- [REST Principles and Architectural Constraints](https://restfulapi.net/rest-architectural-constraints/)

REST (Representational state transfer «передача состояния представления») – соглашение о том, как выстраивать сервисы. Под REST часто имеют в виду т.н HTTP REST API. Как правило, это веб-приложение с набором урлов – конечных точек. Урлы принимают и возвращают данные в формате JSON. Тип операции задают методом HTTP-запроса, например:

- `GET` – получить объект или список объектов
- `POST` – создать объект
- `PUT` – обновить существующий объект
- `PATCH` – частично обновить существующий объект
- `DELETE` – удалить объект
- `HEAD` – получить метаданные объекта

REST-архитектура активно использует возможности протокола HTTP, чтобы избежать т.н. “велосипедов” – собственных решений. Например, параметры кеширования передаются стандартными заголовками `Cache`, `If-Modified-Since`, `ETag`. Авторизациция – заголовком `Authentication`.

REST это архитектурный стиль для проектирования слабо связанных HTTP приложений, что часто используется при разработке веб-сервисов. REST не диктует правил как это должно быть имплементировано на low уровне, он лишь дает высокоуровненые гайдлайны и оставляет тебе свободу для того чтобы воплотить собственную реализацию.

Для веб-служб, построенных с учётом REST (то есть не нарушающих накладываемых им ограничений), применяют термин «RESTful».

В отличие от веб-сервисов (веб-служб) на основе SOAP, не существует «официального» стандарта для RESTful веб-API. Дело в том, что REST является архитектурным стилем, в то время как SOAP является протоколом.

REST определяет 6 архитектурных ограничений, соблюдение которых позволит создать настоящий RESTful API:

1. Единообразие интерфейса
2. Клиент-сервер
3. Отсутствие состояния
4. Кэширование
5. Слои
6. Код по требованию (необязательное ограничение)

**Единообразие интерфейса**
Вы должны придумать API интерфейс для ресурсов системы, доступный для пользователей системы и следовать ему во что бы то ни стало. Ресурс в системе должен иметь только один логичный URI, который должен обеспечивать способ получения связанных или дополнительных данных. Всегда лучше ассоциировать (синонимизировать) ресурс с веб страницей.

Любой ресурс не должен быть слишком большим и содержать все и вся в своем представлении. Когда это уместно, ресурс должен содержать ссылки (HATEOAS: Hypermedia as the Engine of Application State), указывающие на относительные URI для получения связанной информации.

Кроме того, представления ресурсов в системе должны следовать определенным рекомендациям, таким как соглашения об именах, форматы ссылок или формат данных (xml или / и json).

>Как только разработчик ознакомится с одним из ваших API, он сможет следовать аналогичному подходу для других API.

**Клиент-сервер**
По сути, это означает, что клиентское приложение и серверное приложение ДОЛЖНЫ иметь возможность развиваться по отдельности без какой-либо зависимости друг от друга. Клиент должен знать только URI ресурса и больше ничего. Сегодня это нормальная практика в веб-разработке, поэтому с вашей стороны ничего особенного не требуется. Будь проще.

>Серверы и клиенты также могут заменяться и разрабатываться независимо, если интерфейс между ними не изменяется.

**Отсутствие состояния**
Рой Филдинг черпал вдохновение из HTTP, и это отражается в этом ограничении. Сделайте все клиент-серверное взаимодействие без состояний. Сервер не будет хранить информацию о последних  HTTP-запросах клиента. Он будет рассматривать каждый запрос как новый. Нет сессии, нет истории.

Если клиентское приложение должно быть приложением с отслеживанием состояния для конечного пользователя, когда пользователь входит в систему один раз и после этого выполняет другие авторизованные операции, то каждый запрос от клиента должен содержать всю информацию, необходимую для обслуживания запроса, включая сведения об аутентификации и авторизации.

>Клиентский контекст не должен храниться на сервере между запросами. Клиент отвечает за управление состоянием приложения.

**Кэширование**
В современном мире кэширование данных и ответов имеет первостепенное значение везде, где это применимо/возможно. Кэширование повышает производительность на стороне клиента и расширяет возможности масштабирования для сервера, поскольку нагрузка уменьшается.

В REST кэширование должно применяться к ресурсам, когда это применимо, и тогда эти ресурсы ДОЛЖНЫ быть объявлены кешируемыми. Кэширование может быть реализовано на стороне сервера или клиента.

>Хорошо настроенное кэширование частично или полностью исключает некоторые взаимодействия клиент-сервер, что еще больше повышает масштабируемость и производительность.

**Слои**
REST позволяет вам использовать многоуровневую архитектуру системы, в которой вы развертываете API-интерфейсы на сервере A, храните данные на сервере B, a запросы аутентифицируете, например, на сервере C. Клиент обычно не может сказать, подключен ли он напрямую к конечному серверу или к посреднику.

**Код по требованию (необязательное ограничение)**
Это опциональное ограничение. Большую часть времени вы будете отправлять статические представления ресурсов в форме XML или JSON. Но когда вам нужно, вы можете вернуть исполняемый код для поддержки части вашего приложения, например, клиенты могут вызывать ваш API для получения кода визуализации виджета интерфейса пользователя. Это разрешено

Все вышеперечисленные ограничения помогают вам создать действительно RESTful API, и вы должны следовать им. Тем не менее, иногда вы можете столкнуться с нарушением одного или двух ограничений. Не беспокойтесь, вы все еще создаете API RESTful, но не «труЪ RESTful».

### Что такое SOAP

SOAP (от англ. Simple Object Access Protocol - простой протокол доступа к объектам; вплоть до спецификации 1.2) - протокол обмена структурированными сообщениями в распределённой вычислительной среде. Первоначально SOAP предназначался в основном для реализации удалённого вызова процедур (RPC). Сейчас протокол используется для обмена произвольными сообщениями в формате XML, а не только для вызова процедур. Официальная спецификация последней версии 1.2 протокола никак не расшифровывает название SOAP. SOAP является расширением протокола XML-RPC.
SOAP может использоваться с любым протоколом прикладного уровня: SMTP, FTP, HTTP, HTTPS и др. Однако его взаимодействие с каждым из этих протоколов имеет свои особенности, которые должны быть определены отдельно. Чаще всего SOAP используется поверх HTTP.

### В чем разница между REST и SOAP веб сервисами

Некоторые отличия:

- REST поддерживает различные форматы: text, JSON, XML; SOAP - только XML,
- REST работает только по HTTP(S), а SOAP может работать с различными протоколами,
- REST может работать с ресурсами. Каждый URL это представление какого-либо ресурса. SOAP работает с операциями, которые реализуют какую-либо бизнес логику с помощью нескольких интерфейсов,
- SOAP на основе чтения не может быть помещена в кэш, а REST в этом случае может быть закэширован,
- SOAP поддерживает SSL и WS-security, в то время как REST - только SSL, SOAP поддерживает ACID (Atomicity, Consistency, Isolation, Durability). REST поддерживает транзакции, но не один из ACID не совместим с двух фазовым коммитом.

### Можем ли мы посылать SOAP сообщения с вложением

Да, это возможно. Можно посылать вложением различные форматы: PDF, изображения или другие двоичные данные. Сообщения SOAP работают вместе с расширением MIME, в котором предусмотрено multipart/related

### Как бы вы решили какой из REST или SOAP веб сервисов использовать

REST против SOAP можно перефразировать как "Простота против Стандарта". В случае REST (простота) у вас будет скорость, расширяемость и поддержка многих форматов. В случае с SOAP у вас будет больше возможностей по безопасности (WS-security) и транзакционная безопасность (ACID).

## Какие способы для мониторинга веб-приложений в production вы использовали или знаете

- [51 инструмент для APM и мониторинга серверов](https://habr.com/ru/company/pc-administrator/blog/304356/)

# HTTP

## Как устроен протокол HTTP

HTTP – текстовый протокол, работающий поверх TCP/IP. HTTP состоит из запроса и ответа. Их структуры похожи: стартовая строка, заголовки, тело ответа.

Стартовая строка запроса состоит из метода, пути и версии протокола:

```plaintext
GET /index.html HTTP/1.1
```

Стартовая строка ответа состоит из версии протокола, кода ответа и текстовой расшифровке ответа.

```plaintext
HTTP/1.1 200 OK
```

Заголовки – это набор пар ключ-значение, например, `User-Agent`, `Content-Type`. В заголовках передают метаданные запроса: язык пользователя, авторизацию, перенаправление. Заголовок `Host` должен быть в запросе всегда.

Тело ответа может быть пустым, либо может передавать пары переменных, файлы, бинарные данные. Тело отделяется от заголовков пустой строкой.

## Написать raw запрос главной Яндекса

```plaintext
GET / HTTP/1.1
Host: ya.ru
```

## Как клиенту понять, удался запрос или нет

Проверить статус ответа. Ответы разделены старшему разряду. Имеем пять групп со следующей семантикой:

- `1xx`: используется крайне редко. В этой группе только один статус `100 Continue`.
- `2xx`: запрос прошел успешно (данные получены или созданы)
- `3xx`: перенаправление на другой ресурс
- `4xx`: ошибка по вине пользователя (нет такой страницы, нет прав на доступ)
- `5xx`: ошибка по вине сервера (ошибка в коде, сети, конфигурации)

## Что нужно отправить браузеру, чтобы перенаправить на другую страницу

Минимальный ответ должен иметь статус `301` или `302`. Заголовок `Location` указывает адрес ресурса, на который следует перейти.

В теле ответа можно разместить `HTML` со ссылкой на новый ресурс. Тогда пользователи старых браузеров смогут перейти вручную.

## Как управлять кешированием в HTTP

Существуют несколько способов кешировать данные на уровне протокола.

- Заголовки `Cache` и `Cache-Control` регулируют сразу несколько критериев кеша: время жизни, политику обновления, поведение прокси-сервера, тип данных (публичные, приватные).
- Заголовки `Last-Modified` и `If-Modified-Since` задают кеширование в зависимости от даты обновления документа.
- Заголовок `Etag` кеширует документ по его уникальному хешу.

## Как кэшируются файлы на уровне протокола

Когда `Nginx` отдает статичный файл, он добавляет заголовок `Etag` – `MD5`-хеш файла. Клиент запоминает этот хеш. В следующий раз при запросе файла клиент посылает хеш. Сервер проверяет хеш клиента для этого файла. Если хеш не совпадает (файл обновили), сервер отвечает с кодом `200` и выгружает актуальный файл с новым хешем. Если хеши равны, сервер отвечает с кодом `304 Not Modified` с пустым телом. В этом случае браузер подставляет локальную копию файла.

## Что такое HTTP

- [Простым языком об HTTP](https://habr.com/ru/post/215117/)
- [Обзор протокола HTTP - HTTP](https://developer.mozilla.org/ru/docs/Web/HTTP/Overview)

HTTP — широко распространённый протокол передачи данных, изначально предназначенный для передачи гипертекстовых документов (то есть документов, которые могут содержать ссылки, позволяющие организовать переход к другим документам).

Аббревиатура HTTP расшифровывается как HyperText Transfer Protocol, «протокол передачи гипертекста». В соответствии со спецификацией OSI, HTTP является протоколом прикладного (верхнего, 7-го) уровня. Актуальная на данный момент версия протокола, HTTP 1.1, описана в спецификации RFC 2616.

Протокол HTTP предполагает использование клиент-серверной структуры передачи данных. Клиентское приложение формирует запрос и отправляет его на сервер, после чего серверное программное обеспечение обрабатывает данный запрос, формирует ответ и передаёт его обратно клиенту. После этого клиентское приложение может продолжить отправлять другие запросы, которые будут обработаны аналогичным образом.

Задача, которая традиционно решается с помощью протокола HTTP — обмен данными между пользовательским приложением, осуществляющим доступ к веб-ресурсам (обычно это веб-браузер) и веб-сервером. На данный момент именно благодаря протоколу HTTP обеспечивается работа Всемирной паутины.

Также HTTP часто используется как протокол передачи информации для других протоколов прикладного уровня, таких как SOAP, XML-RPC и WebDAV. В таком случае говорят, что протокол HTTP используется как «транспорт».

## Чем отличаются HTTP и HTTPS

HTTP — прикладной протокол передачи данный, используемый для получения информации с веб-сайтов.

HTTPS — расширение протокола HTTP, поддерживающее шифрование по протоколам SSL и TLS.



# Frontend

## Что такое куки. Зачем они, как с ними работать и где они сохраняются

Куки являются информацией, сохраняемой на компьютере веб-сайтом. Куки часто хранят настройки для веб-сайта, например предпочитаемый язык или местоположение. При возвращении на сайт, браузер отправляет обратно куки, которые принадлежат этому сайту. Это позволяет сайту запоминать информацию о предыдущих посещениях.

Django использует куки чтобы хранить идентификатор сессии (или позволяет настроить проект чтобы хранить сессию в куках)

Куки хранятся в браузере.

С ними можно работать как из Django (request.COOKIES, response.st_cookie) так и из JavaScript (document.cookie) (если не установлен флаг HTTPONLY).

## Может ли сервер изменить (добавить, удалить) куки

Да. Значение куки может быть изменено сервером путём отправления новых строк Set-Cookie: name=newvalue. После этого браузер заменяет старое куки с тем же name на новую строку.

## Что такое JWT (JSON Web Token)

- [JWT простым языком: что такое JSON токены и зачем они нужны](https://proglib.io/p/json-tokens/)

Веб-токен JSON, или JWT (произносится «jot»), представляет собой стандартизированный, в некоторых случаях подписанный и/или зашифрованный формат упаковки данных, который используется для безопасной передачи информации между двумя сторонами.

JWT определяет особую структуру информации, которая отправляется по сети. Она представлена в двух формах – сериализованной и десериализованной. Первая используется непосредственно для передачи данных с запросами и ответами. С другой стороны, чтобы читать и записывать информацию в токен, нужна его десериализация.

# SDLC

## Agile/Scrum

- [Agile/Scrum. Все что необходимо знать](https://www.slideshare.net/ssuser2ea1b3/agilescrum-72029120)

## Какая разница между CI и CD

- [Непрерывная интеграция, непрерывная доставка, непрерывное развертывание: просто матрешка](https://habr.com/ru/company/piter/blog/343270/)

**Continuous integration (непрерывная интеграция)**
Непрерывной интеграция заключается в следующем: все изменения, вносимые в код, объединяются в центральном репозитории (операция называется «слияние»). Слияние происходит несколько раз в день, и после каждого слияния в конкретном проекте срабатывает автоматическая сборка и тестирование.

Бывает, что перед сборкой и тестированием программу требуется скомпилировать (это зависит от языка, на котором она написана). Сегодня все чаще возникает необходимость упаковать приложение в контейнер Docker. Затем автоматические тесты проверяют конкретные модули кода, работу UI, производительность приложения, надежность API и пр. Все эти этапы в совокупности обычно называют «сборкой».

CI – это своеобразная страховочная сетка, позволяющая разработчикам избежать массы проблем перед сдачей проекта.

**Continuous delivery (непрерывная доставка)**
Непрерывная доставка – это практика автоматизации всего процесса релиза ПО. Идея заключается в том, чтобы выполнять CI, плюс автоматически готовить и вести релиз к продакшену. При этом желательно добиться следующего: любой, кто обладает достаточными привилегиями для развертывания нового релиза может выполнить развертывание в любой момент, и это можно сделать в несколько кликов. Программист, избавившись практически от всей ручной работы, трудится продуктивнее.

Как правило, в процессе непрерывной доставки требуется выполнять вручную как минимум один этап: одобрить развертывание в продакшен и запустить его. В сложных системах с множеством зависимостей конвейер непрерывной доставки может включать дополнительные этапы, выполняемые вручную либо автоматически.

**Continuous deployment(непрерывное развёртываение)**
Непрерывное развертывание располагается «на уровень выше» непрерывной доставки. В данном случае все изменения, вносимые в исходный код, автоматически развертываются в продакшен, без явной отмашки от разработчика. Как правило, задача разработчика сводится к проверке запроса на включение (pull request) от коллеги и к информированию команды о результатах всех важных событий.

Непрерывное развертывание требует, чтобы в команде существовала отлаженная культура мониторинга, все умели держать руку на пульсе и быстро восстанавливать систему.

Разработчики, практикующие CI и желающие перейти к непрерывному развертыванию, для начала автоматизируют развертывание в обкаточную среду, а развертывание в продакшен продолжают делать вручную – одним кликом.

**Резюме:**

- Непрерывная интеграция (CI): короткоживущие функциональные ветки, команда сливает их с основной веткой разработки по несколько раз в день, процессы сборки и тестирования полностью автоматизированы, результат имеем в пределах 10 минут; развертывание выполняется вручную.
- Непрерывная доставка (CD): автоматизируется CI + весь процесс релиза ПО. Может состоять из нескольких этапов. Развертывание в продакшен выполняется вручную.
- Непрерывное развертывание: CI + CD + полностью автоматизированное развертывание в продакшен.

## Какая разница между Scrum и Kanban

- [Scrum vs Kanban: в чем разница и что выбрать?](https://habr.com/ru/company/hygger/blog/351048/)

Scrum и Kanban — представители методологий Agile-семейства. Обе считаются гибкими и итеративными.

Более 17 лет назад лидеры IT-разработки сформулировали манифест Agile. Главное, что можем выделить из манифеста:

- Люди и взаимодействие важнее процессов и инструментов.
- Работающий продукт важнее исчерпывающей документации.
- Сотрудничество с заказчиком важнее согласования условий контракта.
- Готовность к изменениям важнее следования первоначальному плану.

Основу **Scrum** составляют короткие итерации или спринты, как правило, 2-3-х недельные. Перед началом спринта команда сама формирует список фич на итерацию, далее запускается спринт.

После окончания спринта выполненные фичи заливаются на продакшн, а невыполненные — переносятся в другой спринт. Как правило, фичи, которые делаются во время спринта, не меняются: что было на старте спринта — должно быть сделано любой ценой к окончанию спринта.

**Kanban** дает больше гибкости, если под гибкостью понимать частоту смены приоритетов. Вчера вы залили на прод новую фичу, а сегодня получили данные с передовой и узнали, что вот эта штука не работает так, как было задумано — люди не нажимают кнопку «купить». Вы «даете по шапке» UX, он дает вам новые требования. Вы поднимаете наверх очереди эту задачу, программист берет эту задачу «сверху», выполняет ее и, к вечеру fix уже на проде, конверсия в платежи выросли на 12%. Это победа.

**Основная разница между Scrum и Канбан — в длине итераций. В Scrum итерации — 2 недели, в Kanban задачи программисту можно «подсовывать» хоть каждый день.**

В **Scrum** задачи принято оценивать в Story points или в часах. Без оценки не получится сформировать спринт: ведь нам нужно знать, успеем ли мы сделать задачи за 2 недели. Через 2 недели мы получаем ценную статистику — сколько часов или Story points команда смогла сделать за спринт. Velocity — это производительность команды за один спринт. Этот параметр позволяет Scrum менеджеру предсказать, где команда будет через 2 недели.

В **Kanban** не принято делать оценку. Это опционально, команда решает сама. Здесь нет понятия «скорость работы команды», считается только среднее время на задачу. Время это считается с помощью специального отчета — Cycle Time.

**Итак, в Scrum наша цель — закончить спринт, в Kanban — задачу.**

Scrum — это автобус, который останавливается лишь на определенных остановках, где люди выходят группами. А Kanban — это маршрутка: захотел пассажир выйти, попросил водителя и вышел там, где ему нужно.

## Вопрос для тим-лидов: что Вы будете делать, если на проекте нет тестов и заказчик не хочет тратить на их разработку время и деньги

Апелировать к прибыльности для бизнеса заказчика.

## Что такое Code Debt и как с ним быть

В классическом понимании, т.е. в том виде, в котором эта метафора была описана Вардом Каннингемом, под техническим долгом понимается осознанное компромиссное решение, когда заказчик и ключевые разработчики четко понимают все преимущества от быстрого, пусть и не идеального технического решения, за которое придется расплатиться позднее. И хотя с точки зрения многих разработчиков ситуация, когда плохое решение может быть хорошим, может показаться безумной, на самом деле, это вполне возможно: если краткосрочное решение позволит компании получить видимые преимущества, выпустить продукт раньше конкурентов, удовлетворить ключевого заказчика или каким-то другим образом получить преимущества перед конкурентами, тогда такое решение совершенно оправданно. Иногда это может быть единственным способом, чтобы долгосрочная перспектива вообще существовала.

Технический долг в классическом понимании является преднамеренным и в основном касается стратегических решений, поэтому и ответственность за него лежит на заказчиках, матерых лидах, архитекторах и даже ПМ-ах, но вот все, что связано с грязным кодом касается по большей части простых разработчиков. На самом деле, разница между грязным кодом и неоптимальным стратегическим решением, по сути, не такая уж и большая: при добавлении новой возможности в систему вам приходится расплачиваться за недальновидность в прошлом.

Выплачивать проценты по техдолгу легче, если начать задумываться об этом на этапе создания продукта.

- Делайте одноразовые прототипы MVP и только подтвержденные гипотезы включайте в работу.
- Создавайте архитектуру, которую будет просто изменить: микросервисы, API с версионированием.
- Откажитесь от организации собственных серверных в пользу облачных решений. Например, Microsoft Azure, AWS Amazon Cloud, «Яндекс.Облако», облако от Mail.ru и так далее.
- Установите в команде четкий definition of done, включающий в том числе метрики качества. Очень помогает включить в DOD пункт, исключающий «приемку» фичи, если есть связанные с ней баги.
- Для MVP хорошо проектировать систему так, чтобы миграция пользователей с прототипов на стабильную часть была незаметной.

Компании справляются с техдолгом по-разному. Основных стратегий три.

- Переписывают все с нуля. Это ультимативный способ поддерживать систему в состоянии, когда она постоянно готова к изменениям, если все зашло слишком далеко и уже нет прежней гибкости.
- Делают постепенный рефакторинг. Задачи по техдолгу отправляются в бэклог наравне с продуктовыми задачами. Это замедляет работу по выкатке новых фич, но бизнес обычно идет на компромиссы.
- Смиряются с техдолгом. Если у вас не стартап, а обновления нужны раз в полгода, то можно просто смириться с тем, что код неоптимален, и действовать по принципу «работает — не трогай». Как только поймете, что ошиблись, вы так или иначе переместитесь к пункту 1.

# VCS

- [Git - Book](https://git-scm.com/book/ru/v2)

## Что такое Git Flow

- [Gitflow Workflowl](https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow)
- [Read Git Flow](https://leanpub.com/git-flow/read)

**Historical Branches**
Вместо использования только одной master ветки, этот workflow использует две ветки, для записи истории проекта. Master хранит официальную историю релизов, в development же ветке ведется активная разработка. Также важно тагать комиты в master ветки номерами версий.

**Feature Branches**
Каждая новая фича должна разрабатываться отдельно - в своей ветке, которая должна пушиться в центральный репозиторий для сохранения работы или коллаборации с другими разработчиками. Для создания фичебранча используется development ветка. Когда разработка фичи закончена, она сливается в development. Фичи никогда не должны сливаться напрямую с master веткой.

**Release Branches**
Как только development наберет достаточно фич для релиза (или в заранее определенную дату) создается release ветка из development ветки.

Создание этого бранча начинает новый релиз-цикл, поэтому в эту ветку уже не должно покадать никаких новых фич - только баг-фиксы, генерация документации и другие таски, ориентированные на релиз.

Как только релиз-ветка подготовлена, ее сливают с master и тагают новой версией. Также она должна быть слита и с development веткой, которая уже скорее всего убежала вперед, с момента инициализации релиза.

Использование отдельной ветки для релиза позволяет одной команде готовить релиз, в том время когда другая команда будет продолжать разрабатывать новые фичи в development для последующих релизов.

**Hotfix Branches**
Hotfix ветки используются для быстрого патчинга релизов. Это единственная ветка, которая создается из master. Как только фикс готов, он должен быть слит как в master, так и в development (или в новый release, если она есть) и master должен быть тегирован новой версией (инкрементирована patch цифра. v.1.0.1, например)

Это позволяет исправлять ошибки не прерывая весь воркфлоу или не дожидаясь следующего релиза, для выкатывания исправлений.

## Что такое Git Rebase

Итак git работает с комитами. Каждый комит — набор изменений. У каждого комита есть уникальный hash. Когда происходит слияние веток посредством merge:

`$ git merge "another_branch"`

то все комиты сохраняются — сохраняются комментарии комита, его hash + как правило добавляется еще один искусственный комит. При этом комиты могут чередоваться друг с другом. Это не всегда удобно. Допустим ваш комит решили откатить — выискивать в общем списке где ваш комит, а где не ваш не очень приятно. И вообще — в общей истории хочется видеть действительно важные изменения, а не «ой, я забыл поставить ;». Для того, чтобы несколько комитов склеивать в один можно использовать rebase. Хотя в интерфейсе GitHub есть кнопочка squash & commit — это когда вы создаете pull request (PR) из одной ветки в другую (как правило из вашей рабочей ветки в основную) и после прохождения всех формальностей можно нажать squash & commit, обновить комментарий и ваши изменения появятся в основной ветке как один комит.

Хочу написать о двух случаях использования rebase:

1. Когда изменения включаются из одну ветку в другую не посредством merge, а посредством rebase:
    `$ git rebase "another_branch"`
    Это позволяет ваши локальные комиты поставить после всех комитов, которые были внесены в ветку «another_branch». Хэши ваших комитов изменятся.
2. Когда можно руками отредактировать несколько ваших комитов — например склеить их, изменить коментарий:
    `$ git rebase -i {HEAD~_commit_count_|commit_hash}`

Итак вы все сделали в своей уютненькой веточки и решили поделиться этим комитом с миром, но мир хочет от вас только один комит. `git rebase -i` запустит редактор и предложит отредактировать комиты (порядок следования комитов — сверху вниз в отличие от git log). Можно оставить комит как есть, можно изменить комментарий, можно склеить с предыдущим. Как правило ваш первый комит надо оставить как есть, а все остальные изменить на
`pick "commit_hash" "comment" → fixup "commit_hash" "comment"`.

При этом все комментарии, которые были в fixup комитах потеряются и будет использоваться комментарий от первого комита. Если вам были дороги комментарии, то стоит использовать squash вместо fixup.

Но если процесс разработки был долог, то скорее всего вам приходилось делать merge основной ветки. И все ваши комиты перемешаются с общими комитами и склеивать ваши с не вашими будет задачей непростой. Поэтому перед тем, как делать `git rebase -i <>` стоит сделать `git rebase`. `git rebase` поставит все ваши комиты в конец списка всех комитов (в чем можно убедиться запустив `git log`) и после этого запустиь `git rebase -i <HEAD~Количесво_ваших_комитов>`, во всех строчках кроме первой заменить pick → {fixup|squash} и вуаля — у вас один комит.

Если в процессе редактирования комитов `git rebase -i <>` вы как-то накосячили, то не стоит жать Control+C — exit code выхода из редактора git не волнует. Он просто возьмет файл и сделает все по нему. Просто удалите или закомментируйте все строчки в файле. git поймет, что вы ничего не хотели.

После манипуляций с rebase потребуется push с опцией -F. Все это потому, что мы переписываем меняем историю комитов и git нас об этом честно предупреждает.
`$ git push -f`

## Что такое Git Cherry pick

Команда `git cherry-pick` используется для перенесения отдельных коммитов из одного места репозитория в другое, обычно между ветками разработки и обслуживания. Этот механизм отличается от привычных команд `git merge` и `git rebase`, которые переносят коммиты целыми цепочками.

`git cherry-pick <commit-hash>`

## Что такое force push

Если вы поправили какие-нибудь старые коммиты в истории git, например: исправили имя автора или e-mail, или отменили последний коммит или воспользовались amend или revert, то при попытке push-а git справедливо «ругнётся»

Чтобы нам всё же запушить наши изменения, нам нужно выполнить либо

`git push --force origin <имя_ветки>`

Но в этом случае мы рискуем перетереть чьи-нибудь изменения, если с тех пор, как мы забирали изменения с сервера, кто-то успел запушить свои коммиты. Поэтому лучше использовать более безопасную команду:

`git push --force-with-lease origin <имя_ветки>`

Такой вариант лучше тем, что если кто-то успел запушить свои коммиты после того, как мы забирали изменения с сервера, то он не будет их перетирать, а выдаст нам ошибку, после чего мы сможем интегрировать чужие коммиты со своими изменениями и попытаться сделать `push --force-with-lease` ещё раз.

## Что такое pre-commit check

Как и во многих других системах контроля версий, в Git'е есть возможность запускать собственные сценарии в те моменты, когда происходят некоторые важные действия. Существуют две группы подобных перехватчиков (hook): на стороне клиента и на стороне сервера. Перехватчики на стороне клиента предназначены для клиентских операций, таких как создание коммита и слияние. Перехватчики на стороне сервера нужны для серверных операций, таких как приём отправленных коммитов. Перехватчики могут быть использованы для выполнения самых различных задач. О некоторых из таких задач мы и поговорим.

Pre-commit check можно использовать например так:

- выполнять проверку кода на валидность (например: соответствие требованиям PEP8, наличие документации итд);
- выполнять комплексную проверку проекта (юнит-тесты итд);
- прерывать операцию commit'а в случае обнаружения ошибок и отображать подробный журнал для разбора полетов.

# БД

- [Топ-65 вопросов по SQL с собеседований, к которым вы должны подготовиться в 2019 году. Часть I](https://habr.com/ru/company/otus/blog/461067/)

## Что такое транзакция. Какие у неё есть свойства

- [Руководство по SQL. Транзакции](https://proselyte.net/tutorials/sql/sql-transactions/)

Транзакция является рабочей единицей работы с базой данных (далее – БД). Это последовательность операций, выполняемых в логическом порядке пользователем, либо программой, которая работает с БД.

Мы можем сказать, что транзакция – это распространение изменений в БД. Например, если мы создаём, изменяем или удаляем запись, то мы выполняем транзакцию. Крайне важно контролировать транзакции для гарантирования.

Основные концепции (свойства) транзакции описываются аббревиатурой ACID – Atomicity, Consistency, Isolation, Durability (Атомарность, Согласованность, Изолированность, Долговечность).

**Атомарность:**

Атомарность гарантирует, что любая транзакция будет зафиксирована только целиком (полностью). Если одна из операций в последовательности не будет выполнена, то вся транзакция будет отменена. Тут вводится понятие “отката” (rollback). Т.е. внутри последовательности будут происходить определённые изменения, но по итогу все они будут отменены (“откачены”) и по итогу пользователь не увидит никаких изменений.

**Согласованность:**

Это означает, что любая завершённая транзакция (транзакция, которая достигла завершения транзакции – end of transaction) фиксирует только допустимые результаты. Например, при переводе денег с одного счёта на другой, в случае, если деньги ушли с одного счёта, они должны прийти на другой (это и есть согласованность системы). Списание и зачисление  – это две разные транзакции, поэтому первая транзакция пройдёт без ошибок, а второй просто не будет. Именно поэтому крайне важно учитывать это свойство и поддерживать баланс системы.

**Изолированность:**

Каждая транзакция должна быть изолирована от других, т.е. её результат не должен зависеть от выполнения других параллельных транзакций. На практике, изолированность крайне труднодостижимая вещь, поэтому здесь вводится понятие “уровни изолированности” (транзакция изолируется не полностью).

**Долговечность:**

Эта концепция гарантирует, что если мы получили подтверждение о выполнении транзакции, то изменения, вызванные этой транзакцией не должны быть отменены из-за сбоя системы (например, отключение электропитания).

## Какие команды управления транзакциями вы знаете

- [Руководство по SQL. Транзакции](https://proselyte.net/tutorials/sql/sql-transactions/)

Для управления транзакциями используются следующие команды:

- COMMIT: Сохраняет изменения
- ROLLBACK: Откатывает (отменяет) изменения
- SAVEPOINT: Создаёт точку к которой группа транзакций может откатиться
- SET TRANSACTION: Размещает имя транзакции.

Команды управление транзакциями используются только для DML команд: INSERT, UPDATE, DELETE. Они не могут быть использованы во время создания, изменения или удаления таблицы.

## Что такое уровни изолированности транзакций. Какие они бывают

"Высоконагруженные приложения". М. Клеппман. Глава 7

*Изоляция* в смысле ACID означает, что конкурентно выполняемые транзакции изолированы друг от друга — они не могут помешать друг другу. Классические учебники по базам данных понимают под изоляцией сериализуемость (serializability). То есть каждая транзакция выполняется так, будто она единственная во всей базе. БД гарантирует, что результат фиксации транзакций такой же, как если бы они выполнялись последовательно (serially, одна за другой), хотя в реальности они могут выполняться конкурентно.

**Чтение зафиксированных данных (read comitted):**

Самый базовый уровень изоляции транзакций — чтение зафиксированных данных. Он обеспечивает две гарантии.

- При чтении из БД клиент видит только зафиксированные данные (никаких «грязных» операций чтения).
- При записи в БД можно перезаписывать только зафиксированные данные (никаких «грязных» операций записи).

*«Грязные» операции чтения*. Клиент читает записанные другим клиентом данные до их фиксации. Уровень изоляции чтения зафиксированных данных и более сильные предотвращают «грязные» операции чтения.

*«Грязные» операции записи*. Клиент перезаписывает данные, которые другой клиент записал, но еще не зафиксировал. Практически все реализации транзакций предотвращают «грязные» операции записи.

**Изоляция снимков состояния и воспроизводимое чтение:**

Ее идея состоит в том, что каждая из транзакций читает данные из согласованного снимка состояния БД, то есть видит данные, которые были зафиксированы в базе на момент ее (транзакции) начала. Даже если данные затем были изменены другой транзакцией, каждая транзакция видит только старые данные, по состоянию на конкретный момент времени. Позволяет предотвратить асимметрии чтения.

*Асимметрия чтения* (невоспроизводимое чтение). Клиент видит различные части базы данных по состоянию на разные моменты времени. Чаще всего такую проблему предотвращают с помощью изоляции снимков состояния, при которой транзакция читает данные из согласованного снимка состояния, соответствующего определенному моменту времени. Обычно это реализуется благодаря многоверсионному управлению конкурентным доступом (MVCC).

**Сериализуемость (serializability):**

Обычно считается самым сильным уровнем изоляции. Она гарантирует, что даже при конкурентном выполнении транзакций результат останется таким же, как и в случае их последовательного (по одной за раз) выполнения, без всякой конкурентности. Следовательно, база данных гарантирует, что правильно выполняющиеся последовательно транзакции будут столь же правильно выполняться конкурентно. Другими словами, база предотвращает все возможные состояния гонки.

Большинство современных БД, обеспечивающих сериализуемость, применяют один из трех методов:

- По-настоящему последовательное выполнение транзакций. Если вы можете сделать отдельные транзакции очень быстрыми, причем количество транзакций, обрабатываемых за единицу времени на одном ядре CPU, достаточно невелико, то для обработки этот вариант окажется простым и эффективным.
- Двухфазная блокировка. На протяжении десятилетий она была стандартным способом обеспечения сериализуемости, но многие приложения стараются ее не использовать из-за плохих показателей производительности.
- Сериализуемая изоляция снимков состояния (SSI). Довольно свежий алгоритм, лишенный практически всех недостатков предыдущих подходов. В нем используется оптимистический подход, благодаря чему транзакции выполняются без блокировок. Перед фиксацией транзакции выполняется проверка, и если выполнение было несериализуемым, то транзакция прерывается без фиксации.

## Что такое вложенные транзакции

Вложенными называются транзакции, выполнение которых инициируется из тела уже активной транзакции .

Для создания вложенной транзакции пользователю не нужны какие-либо дополнительные команды. Он просто начинает новую транзакцию, не закрыв предыдущую. Завершение транзакции верхнего уровня откладывается до завершения вложенных транзакций. Если транзакция самого нижнего ( вложенного ) уровня завершена неудачно и отменена, то все транзакции верхнего уровня, включая транзакцию первого уровня, будут отменены. Кроме того, если несколько транзакций нижнего уровня были завершены успешно (но не зафиксированы), однако на среднем уровне (не самая верхняя транзакция ) неудачно завершилась другая транзакция, то в соответствии с требованиями ACID произойдет откат всех транзакций всех уровней, включая успешно завершенные. Только когда все транзакции на всех уровнях завершены успешно, происходит фиксация всех сделанных изменений в результате успешного завершения транзакции верхнего уровня.

Каждая команда COMMIT TRANSACTION работает только с последней начатой транзакцией. При завершении вложенной транзакции команда COMMIT применяется к наиболее "глубокой" вложенной транзакции. Даже если в команде COMMIT TRANSACTION указано имя транзакции более высокого уровня, будет завершена транзакция, начатая последней.

## Что такое курсор и зачем он нужен

Запрос к реляционной базе данных обычно возвращает несколько рядов (записей) данных, но приложение за один раз обрабатывает лишь одну запись. Даже если оно имеет дело одновременно с несколькими рядами (например, выводит данные в форме электронных таблиц), их количество по-прежнему ограничено. Кроме того, при модификации, удалении или добавлении данных рабочей единицей является ряд. В этой ситуации на первый план выступает концепция курсора, и в таком контексте курсор – указатель на ряд.

Курсор в SQL – это область в памяти базы данных, которая предназначена для хранения последнего оператора SQL. Если текущий оператор – запрос к базе данных, в памяти сохраняется строка данных запроса, называемая текущим значением, или текущей строкой курсора. Указанная область в памяти поименована и доступна для прикладных программ.

Обычно курсоры используются для выбора из базы данных некоторого подмножества хранимой в ней информации. В каждый момент времени прикладной программой может быть проверена одна строка курсора. Курсоры часто применяются в операторах SQL, встроенных в написанные на языках процедурного типа прикладные программы. Некоторые из них неявно создаются сервером базы данных, в то время как другие определяются программистами.

В некоторых случаях применение курсора неизбежно. Однако по возможности этого следует избегать и работать со стандартными командами обработки данных: SELECT, UPDATE, INSERT, DELETE. Помимо того, что курсоры не позволяют проводить операции изменения над всем объемом данных, скорость выполнения операций обработки данных посредством курсора заметно ниже, чем у стандартных средств SQL.

## Какая разница между PostgreSQL и MySQL

- [Сравнение MySQL и PostgreSQL](https://hyperhost.ua/info/ru/sravnenie-mysql-i-postgresql)

**История разработки MySQL и PostgreSQL.**

MySQL начал создаваться еще в 90-х. Внутренний выпуск произошел в 1995 году. Тогда разработкой MySQL занимались несколько компаний. Начиная с 2010 года компания  Oracle владеет проектом MySQL и разрабатывает новые версии.

PostgreSQL немного ранее в 1986 году начал разрабатываться в Калифорнийском университете.  Над проектом работали более 8 лет, но потом был разделен на коммерческую БД IIlustra и свободный проект Postrgesql.

**Особенности хранения данных.**

В MySQL для хранения данных в таблицах используются различные движки. Движок не имеет влияния на синтаксис запросов и их выполнение. Имеется поддержка MyISAM, InnoDB, MEMORY, Berkeley DB. Их основное отличие в способе записи данных на диск и методов считывания. PostgreSQL работает только на движке storage engine. Таблицы организованы в виде обьектов, а действия выполняются с помощью объективно ориентированных функций.

**Стандарты SQL.**

SQL - это стандартизированный язык выполнения запросов, который используется и MySQL и PostgreSQL. Этот стандарт имеет несколько версий и был разработан еще в 1986 году.

MySQL имеет поддержку не всех функций и возможностей SQL. Это сделано для того, чтобы работать с MySQL было просто и удобно. Но если для проекта необходимо какое-то расширение, разработчики его могут добавить не в ущерб стандарту.

PostgreSQL поддерживает все новые стандарты SQL, из-за этого данный проект довольно сложный и не настолько популярный как MySQL.

**Возможности обработки данных.**

MySQL при исполнении запроса делает загрузку всего ответа сервера в память клиента. В случае больших обьемов это не всегда удобно. По функциям Postgresql более широкий чем Mysql. Например, в Postgresql при помощи курсора можно перемещать полученные данные. Вам предоставляется только указатель, а весь ответ хранится в памяти сервера баз данных. Данный указатель можно хранить между сеансами. Postgresql имеет поддержку регулярных выражений в запросах, рекурсивных запросов и наследования таблиц.

**Производительность MySQL и Postgresql.**

MySQL всегда был ориентирован на большую производительность, в то время как Postgresql был нацелен на большое количество настроек и стандартов. Но со временем эта ситуация поменялась и Postgre стал более производительным.

Для организации работы с базой данных в MySQL используется таблица InnoDB. А это значит, что MySQL будет значительно быстрее Postgre в случае использовании первичного ключа.

По поводу  Postgresql, вся заголовочная информация таблиц размещается в оперативной памяти. Можно применять несколько индексов к одной таблице для большего удобства. В общем PostgreSQL работает быстрее, кроме ситуаций с использованием первичных ключей.

**Поддерживаемые типы данных.**

MySQL и Postgresql имеют похожий набор, который, конечно же, имеет свои отличия. В Postgre типы более разнообразны и есть свои типы полей для определенных видов данных, которых, например, нет в MySQL.

## Что такое VACUUM в PostgreSQL

VACUUM высвобождает пространство, занимаемое «мёртвыми» кортежами. При обычных операциях Postgres кортежи, удалённые или устаревшие в результате обновления, физически не удаляются из таблицы; они сохраняются в ней, пока не будет выполнена команда VACUUM. Таким образом, периодически необходимо выполнять VACUUM, особенно для часто изменяемых таблиц.

## Что такое EXPLAIN. Какая разница между ним и EXPLAIN ANALYZE

EXPLAIN выводит информацию, необходимую для понимания, что же делает ядро при каждом конкретном запросе.

EXPLAIN ANALYZE выполняет объясняемое выражение, даже если это insert, update или delete.

## Какие виды Join'ов вы знаете, чем они отличаются друг от друга

- [Табличные выраженияl](https://postgrespro.ru/docs/postgresql/9.6/queries-table-expressions)

Слова INNER и OUTER необязательны во всех формах. По умолчанию подразумевается INNER (внутреннее соединение), а при указании LEFT, RIGHT и FULL — внешнее соединение.

Условие соединения указывается в предложении ON или USING, либо неявно задаётся ключевым словом NATURAL. Это условие определяет, какие строки двух исходных таблиц считаются «соответствующими» друг другу.

Возможные типы соединений с сопоставлениями строк:

*INNER JOIN*
Для каждой строки R1 из T1 в результирующей таблице содержится строка для каждой строки в T2, удовлетворяющей условию соединения с R1.

*LEFT OUTER JOIN*
Сначала выполняется внутреннее соединение (INNER JOIN). Затем в результат добавляются все строки из T1, которым не соответствуют никакие строки в T2, а вместо значений столбцов T2 вставляются NULL. Таким образом, в результирующей таблице всегда будет минимум одна строка для каждой строки из T1.

*RIGHT OUTER JOIN*
Сначала выполняется внутреннее соединение (INNER JOIN). Затем в результат добавляются все строки из T2, которым не соответствуют никакие строки в T1, а вместо значений столбцов T1 вставляются NULL. Это соединение является обратным к левому (LEFT JOIN): в результирующей таблице всегда будет минимум одна строка для каждой строки из T2.

*FULL OUTER JOIN*
Сначала выполняется внутреннее соединение. Затем в результат добавляются все строки из T1, которым не соответствуют никакие строки в T2, а вместо значений столбцов T2 вставляются NULL. И наконец, в результат включаются все строки из T2, которым не соответствуют никакие строки в T1, а вместо значений столбцов T1 вставляются NULL.

Предложение ON определяет наиболее общую форму условия соединения: в нём указываются выражения логического типа, подобные тем, что используются в предложении WHERE. Пара строк из T1 и T2 соответствуют друг другу, если выражение ON возвращает для них true.

USING — это сокращённая запись условия, полезная в ситуации, когда с обеих сторон соединения столбцы имеют одинаковые имена. Она принимает список общих имён столбцов через запятую и формирует условие соединения с равенством этих столбцов. Например, запись соединения T1 и T2 с USING (a, b) формирует условие ON T1.a = T2.a AND T1.b = T2.b.

Более того, при выводе JOIN USING исключаются избыточные столбцы: оба сопоставленных столбца выводить не нужно, так как они содержат одинаковые значения. Тогда как JOIN ON выдаёт все столбцы из T1, а за ними все столбцы из T2, JOIN USING выводит один столбец для каждой пары (в указанном порядке), за ними все оставшиеся столбцы из T1 и, наконец, все оставшиеся столбцы T2.

Наконец, NATURAL — сокращённая форма USING: она образует список USING из всех имён столбцов, существующих в обеих входных таблицах. Как и с USING, эти столбцы оказываются в выходной таблице в единственном экземпляре. Если столбцов с одинаковыми именами не находится, NATURAL JOIN действует как JOIN ... ON TRUE и выдаёт декартово произведение строк.

Еще есть cross join - декартово произведение.

# Дизайн-интервью

[Как задизайнить Facebook за пол часа или секреты System Design Interview / Алексей Петров](https://www.youtube.com/watch?v=Be7INI_U6GY)

Тема очень обширная, поэтому этот раздел следует воспринимать как чеклист для его прохождения.

## План интервью

Будем считать что тайм-слот интервью - 40 минут.

1. Уточнить требования и ограничения (4 минуты)
2. Сделать эстимейты проектируемой системы (пропускная способность, сколько нужно хранить информации, количество серверов и т.д.) (3 минуты)
3. System interface - какие сервисы предоставляет система, какие сервисы использует система (3 минуты)
4. System high-level design - какие компоненты входят в систему, как они взаимодействуют друг с другом (5 минут)
5. Component detailed design - какие компоненты входят в систему, как они взаимодействуют друг с другом. Описать возможные ботлнеки (15 минут)
6. Масштабирование - как система будет масштабироваться (5 минут)
7. Summary - общий обзор и презентация решения (5 минут)

## 1. Сбор требований

Собираем ответы на вопросы "Что система делает?" и "Какой должна быть система?"

Примеры вопросов:

- Это должна быть глобальная система или региональная?
- Как быстро система должна реагировать на внесенные изменения (latency)?
- Какая должна быть доступность системы (availability)?
- Сколько у нас пользователей активно ежедневно?
- Сколько пользователи генерируют трафика ежедневно (количество постов, публикаций и тд)?
- Какое количество информации пользователь просматривает каждый день?

Если нам предлагают спроектировать систему по примеру существующей (twitter, facebook, google docs, etc), то мы можем спросить:
- Какую часть системы мы проектируем?
- Какие именно функции должны быть реализованы?

Сразу формируем для себя чек-лист требований, чтобы не забыть что-то важное.
Например нам предложили спроектировать Facebook с такими требованиями:

- Дизайним ленту новостей (news feed) фейсбука
- Фото/видео не реализуем
- Ранжирование постов не нужно, хронологический порядок
- Встраиваем реламу в ленту (желательно)
- Глобальная система (multi-region)
- Latency (внутри региона) < 1s
- Latency (между регионами) < 60s
- Durability (постоянство данных) очень важно
- Availability (доступность) менее важно
- Миллиард пользователей
- 10 миллионов постов в день
- 500 друзей в среднем
- 5 просмотров фида на пользователя в день

## 2. Эстимейты

Чтобы посчитать эстимейты нужно примерно представить какой тип информации сколько весит.

Хранение информации:
- Символ - 1 байт
- Метаданные (строка в базе, вес поста, etc) - 5-10 килобайт
- 1080p изображение - 2 мегабайта
- 1080p видео (минута) - 30 мегабайт

Сервера:
- Дисковое пространство - 10 терабайт
- RAM - 256-512 гигабайт

Итого считаем эстимейты для примера с фейсбуком:
- Read-write ratio - 5B / 10M = 500:1
- RPS
  - Read: 5B / (24 * 60 * 60) = ~58k rps
  - Write: 10M / (24 * 60 * 60) = ~115 rps
- Storage:
  - 10KB * 10M = 100GB ежедневно
  - 30 * 100GB = 3TB ежемесячно
- Пропускная способность:
  - Read: 5B * 20 постов * 10KB = 1PB ежедневно
  - Write: 10M * 10KB = 100GB ежедневно

PS. RPS мы посчитали "постоянный", в пиках он может увеличиваться в 10 раз (условно)

## 3. API

Описываем максимально просто - какие методы будут доступны, какие параметры будут принимать.

## 4. High-level design

Не стоит называть какие-то конкретные технологии, а просто описывать какие компоненты будут в системе и как они будут взаимодействовать.
Ну тоесть не нужно прям называть Nginx, а просто описать что будет балансировщик нагрузки.

- Если ставим лоад-балансер, то какой? (Round-robin, sticky sessions, etc)
- Если БД, то какая? (RDBMS, NoSQL, inmemory etc)

## 5. Detailed design

- Описываем схему БД и запросы к ней (можем прям примеры запросов писать)
- Перебираем подходы по обработке данных (pros/cons)
- Выбираем решения и объясняем их tradeoffs
- Проверяем требования (список который мы составили на шаге 1)
- Определить edge cases (если они есть)

### Performance mantras

В процессе, если мы сталкиваемся с проблемой производительности, то мы можем применять следующие [мантры](https://www.brendangregg.com/methodology.html):
- Не делай этого
- Делай, но только один раз
- Делай это реже
- Сделай это позже
- Сделай пока пользователь этого не видит
- Сделай это параллельно
- Сделай это дешевле

## 6. Масштабирование

Если говорим про шардирование, то сразу оговариаем какой ключ шардирования выбираем.

### Performance bottlenecks

В зависимости от количества пользователей нам может понадобиться разные инфраструктурные решения:

- 1000 пользователей
  - 1 сервер
  - 1 БД
- 10 000 пользователей
  - Read replicas
  - Несколько серверов
  - Load balancer(s)
- 100 000 пользователей
  - Message queue
  - Rate limits
  - Cache
  - CDN
- 1 000 000 пользователей
  - Stateless services (если они еще не такие)
  - Возможно появится noSQL (если еще не использовался)
  - Database sharding
- 1 000 000 000 пользователей
  - Regional DCs

# Вопросы работодателю

- [Вопросы не мальчика, а джуна. 22 вопроса работодателю на собеседовании на позицию «Middle Python-разработчик»](https://habr.com/ru/post/428283/)

## Вопросы HR'у

1. Что с отпуском и больничными?
2. Переносится ли отпуск на следующий год?
3. Какое отношение к официальным государственным выходным?
4. Есть ли мед-страховка?
5. Какие ограничения есть на период испытательного срока
6. Свободный график? Нужно находиться в офисе n часов?
7. Как в компании относятся к удалённой работе?
8. Можно ли получить оборудование для удалённой работы (мониторы, etc.) или может есть компенсация на оборудование рабочего места
9. Существует ли в компании полугодовая/годовая оценка сотрудников и как она происходит?
10. Какой минимальный срок для первого пересмотра ЗП начиная с первого рабочего дня?
11. Есть ли у в компании переработки? Если есть, то компенсируются ли они и как часто они происходят?
12. Насколько в компании сильна бюрократия?
13. Является ли компания участником каких-либо IT-конференций и есть ли у компании публикации на IT-темы?
14. Есть ли митапы внутри компании?
15. Есть ли в компании стажёры и развита ли система наставничества?
16. Зарплата в какой валюте и привязана ли к курсу, если не в у.е?

## Вопросы для технического собеседования

1. Как дела с тестированием? Какие тесты вы пишете? Какие библиотеки для тестирования вы используете? (фабрики, моки и т.д.)
2. Есть ли Code Review? Как оно проходит?
3. Есть ли в проектах CI/CD? Есть ли DevOps-инженер?
4. Используете ли вы git-flow или какую-либо определенную методологию при работе с git?
5. Используете ли Вы методологию разработки (scrum, kanban и т.д.)?
6. Используются ли системы мониторинга в проектах(Sentry, NewRelic и т.д.)?
7. Используется ли в проекте система для хранения логов и работы с ними(ELK-технология и прочее)?
8. Какие БД используются в проекте? Почему именно такие?
9. Какая версия языка Python используется в проектах?
10. Компания ищет fullstack-разработчика или backend-разработчика?
11. Используется ли технология контейнеризации в проектах?
12. Немного поспрашивать собеседующего о том, чем он занимался до этого проекта и давно ли он в проекте.
13. Есть ли синьеры в команде или просто очень опытные разрабы?
14. Оцениваниются ли задачи по времени или стори-поинтам?
15. Как легко втащить в проект новую технологию? Как выбирается стек для нового проекта?
16. Как в команде принимаются решения? Есть ли человек за которым последнее слово?
17. Попросить описать несколько типичных задач или несколько последних
18. Какие версии Python и фреймворков используются?
19. Насколько часто приходится работать с legacy?
20. Если приходится работать с legacy то какой подход: вносим минимальные правки или пытаемся привести legacy в нормальный вид?

# Интересные ссылки

- [Интересности и полезности python](https://habr.com/ru/post/421993/)
- [Python: вещи, которых вы могли не знать](https://habr.com/ru/post/207988/)
- [Интересные особенности Python, о которых вы могли не догадываться](https://habr.com/ru/post/322360/)
- [Python на Хабре](https://habr.com/ru/post/205944/)
- [Микросервисы (Microservices)](https://habr.com/ru/post/249183/)
- [Цикл событий в js](https://www.youtube.com/watch?v=8cV4ZvHXQL4&index=3&t=0s&list=PLmcXv3fDgVcjpZ6QnpXrSNTj7-RxtdFhA)
- [Разница между nginx и apache с примерами](https://habr.com/ru/post/320710/)

# Источники вопросов

- [Обширный обзор собеседований по Python. Советы и подсказки](https://habr.com/ru/post/439576/)
- [Собеседование. Вопросы для подготовки](https://grishaev.me/interview/)
